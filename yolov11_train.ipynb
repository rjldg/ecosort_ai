{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adeeebda-1722-4338-90d3-d89009e6699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f3db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "Is CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA_VISIBLE_DEVICES: None\n",
      "Device Name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"Is CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"❌ CUDA not available to PyTorch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8991324-61c1-4557-8a8d-7d23ce17958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt to 'yolo11l.pt': 100%|██████████| 49.0M/49.0M [00:05<00:00, 9.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolo11l.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758ebdfb-3599-4c44-98b7-760e1666da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.176  Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train7, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train7, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=18\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1424902  ultralytics.nn.modules.head.Detect           [18, [256, 512, 512]]         \n",
      "YOLO11l summary: 357 layers, 25,324,358 parameters, 25,324,342 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 131.237.5 MB/s, size: 51.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\CYCU\\ecosort_ai\\train\\labels.cache... 4200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4200/4200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 78.435.9 MB/s, size: 35.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\CYCU\\ecosort_ai\\valid\\labels.cache... 1704 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1704/1704 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train7\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000455, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train7\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      8.58G      1.233      3.095      1.189         31        416: 100%|██████████| 132/132 [17:22<00:00,  7.90s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 27/27 [00:17<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1704       4830      0.169      0.148     0.0753      0.048\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      8.73G      1.272      2.302      1.219         62        416: 100%|██████████| 132/132 [15:08<00:00,  6.88s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 27/27 [00:15<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1704       4830      0.273      0.208      0.136     0.0919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      8.79G      1.259      2.111      1.221         40        416: 100%|██████████| 132/132 [18:36<00:00,  8.46s/it] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 27/27 [00:20<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1704       4830      0.277      0.213      0.148     0.0979\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      8.74G      1.196      1.871      1.185         62        416: 100%|██████████| 132/132 [15:47<00:00,  7.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 27/27 [00:15<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1704       4830      0.328      0.243        0.2      0.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      8.79G      1.072      1.554      1.117         28        416: 100%|██████████| 132/132 [15:12<00:00,  6.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 27/27 [00:15<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1704       4830      0.445       0.26      0.245      0.178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 1.396 hours.\n",
      "Optimizer stripped from runs\\detect\\train7\\weights\\last.pt, 51.2MB\n",
      "Optimizer stripped from runs\\detect\\train7\\weights\\best.pt, 51.2MB\n",
      "\n",
      "Validating runs\\detect\\train7\\weights\\best.pt...\n",
      "Ultralytics 8.3.176  Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,293,190 parameters, 0 gradients, 86.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 27/27 [00:15<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1704       4830      0.448      0.259      0.245      0.178\n",
      "        Aluminium foil         48         62      0.399      0.387      0.391      0.316\n",
      "                Bottle        347        459      0.404      0.606      0.538      0.392\n",
      "            Bottle cap        275        320      0.667      0.338      0.425      0.266\n",
      "          Broken glass         13        123          1          0    0.00613    0.00555\n",
      "                   Can        187        267      0.593      0.562       0.57      0.442\n",
      "                Carton        207        263      0.399      0.312      0.289      0.214\n",
      "             Cigarette        223        565      0.687     0.0336     0.0738     0.0436\n",
      "                   Cup        162        186      0.367      0.484      0.415      0.303\n",
      "                   Lid         82         93      0.502      0.323      0.297       0.24\n",
      "          Other litter        148        178      0.364      0.112      0.112     0.0777\n",
      "         Other plastic        181        265      0.395     0.0468     0.0837     0.0579\n",
      "                 Paper        126        178      0.231     0.0843     0.0831      0.062\n",
      " Plastic bag - wrapper        580        854      0.297      0.427      0.289      0.181\n",
      "     Plastic container         83         90      0.355      0.211       0.22      0.169\n",
      "               Pop tab         87        125      0.363      0.104      0.108     0.0654\n",
      "                 Straw        112        120      0.388      0.307      0.247      0.173\n",
      "       Styrofoam piece         77        113      0.304      0.301      0.189      0.163\n",
      "      Unlabeled litter        291        569      0.355     0.0193     0.0652     0.0317\n",
      "Speed: 0.1ms preprocess, 5.7ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='data.yaml', epochs=5, imgsz=416, batch=32, device='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a859533",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = YOLO('runs/detect/train7/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a0d15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://responsiblecafes.org/wp-content/uploads/trash4treasure.jpg to 'trash4treasure.jpg': 100%|██████████| 125k/125k [00:00<00:00, 1.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 d:\\CYCU\\ecosort_ai\\trash4treasure.jpg: 288x416 3 Cartons, 2 Cups, 3 Lids, 1 Other litter, 1 Paper, 1 Plastic bag - wrapper, 2 Plastic containers, 1 Styrofoam piece, 173.2ms\n",
      "Speed: 3.6ms preprocess, 173.2ms inference, 14.5ms postprocess per image at shape (1, 3, 288, 416)\n"
     ]
    }
   ],
   "source": [
    "results = trained_model(\"https://responsiblecafes.org/wp-content/uploads/trash4treasure.jpg\")\n",
    "\n",
    "# Access the results\n",
    "for result in results:\n",
    "    xywh = result.boxes.xywh  # center-x, center-y, width, height\n",
    "    xywhn = result.boxes.xywhn  # normalized\n",
    "    xyxy = result.boxes.xyxy  # top-left-x, top-left-y, bottom-right-x, bottom-right-y\n",
    "    xyxyn = result.boxes.xyxyn  # normalized\n",
    "    names = [result.names[cls.item()] for cls in result.boxes.cls.int()]  # class name of each box\n",
    "    confs = result.boxes.conf  # confidence score of each box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d97d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://responsiblecafes.org/wp-content/uploads/trash4treasure.jpg locally at trash4treasure.jpg\n",
      "image 1/1 d:\\CYCU\\ecosort_ai\\trash4treasure.jpg: 448x640 2 Cartons, 1 Cup, 2 Lids, 4 Other litters, 1 Plastic bag - wrapper, 3 Plastic containers, 1 Straw, 2 Styrofoam pieces, 202.4ms\n",
      "Speed: 4.0ms preprocess, 202.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\predict\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'Aluminium foil', 1: 'Bottle', 2: 'Bottle cap', 3: 'Broken glass', 4: 'Can', 5: 'Carton', 6: 'Cigarette', 7: 'Cup', 8: 'Lid', 9: 'Other litter', 10: 'Other plastic', 11: 'Paper', 12: 'Plastic bag - wrapper', 13: 'Plastic container', 14: 'Pop tab', 15: 'Straw', 16: 'Styrofoam piece', 17: 'Unlabeled litter'}\n",
       " obb: None\n",
       " orig_img: array([[[70, 73, 77],\n",
       "         [70, 73, 77],\n",
       "         [70, 73, 77],\n",
       "         ...,\n",
       "         [35, 40, 39],\n",
       "         [34, 39, 38],\n",
       "         [34, 39, 38]],\n",
       " \n",
       "        [[70, 73, 77],\n",
       "         [70, 73, 77],\n",
       "         [70, 73, 77],\n",
       "         ...,\n",
       "         [35, 40, 39],\n",
       "         [35, 40, 39],\n",
       "         [34, 39, 38]],\n",
       " \n",
       "        [[70, 73, 77],\n",
       "         [70, 73, 77],\n",
       "         [70, 73, 77],\n",
       "         ...,\n",
       "         [36, 41, 40],\n",
       "         [35, 40, 39],\n",
       "         [35, 40, 39]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[54, 59, 58],\n",
       "         [55, 60, 59],\n",
       "         [55, 60, 59],\n",
       "         ...,\n",
       "         [35, 40, 39],\n",
       "         [34, 39, 38],\n",
       "         [33, 38, 37]],\n",
       " \n",
       "        [[54, 59, 58],\n",
       "         [54, 59, 58],\n",
       "         [55, 60, 59],\n",
       "         ...,\n",
       "         [38, 43, 42],\n",
       "         [37, 42, 41],\n",
       "         [36, 41, 40]],\n",
       " \n",
       "        [[56, 61, 60],\n",
       "         [57, 62, 61],\n",
       "         [58, 63, 62],\n",
       "         ...,\n",
       "         [42, 47, 46],\n",
       "         [41, 46, 45],\n",
       "         [40, 45, 44]]], shape=(1281, 1920, 3), dtype=uint8)\n",
       " orig_shape: (1281, 1920)\n",
       " path: 'd:\\\\CYCU\\\\ecosort_ai\\\\trash4treasure.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs\\\\predict'\n",
       " speed: {'preprocess': 4.039299965370446, 'inference': 202.4129000492394, 'postprocess': 1.3917000032961369}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model(\n",
    "    \"https://responsiblecafes.org/wp-content/uploads/trash4treasure.jpg\",\n",
    "    save=True, show=True, imgsz=640, conf=0.25, iou=0.5, project=\"runs\", name=\"predict\", exist_ok=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eec7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecosort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
